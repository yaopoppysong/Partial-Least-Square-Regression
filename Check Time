# Check time of the code

# load package
import cProfile, pstats
import numpy as np
import numpy.random as npr
from sklearn.preprocessing import scale
from numpy import linalg as LA
from io import StringIO

# Set initial data set
npr.seed(123)
X = np.random.normal(0, 1, (10, 3))
Y = np.random.normal(0, 1, (10, 2))
# normalize the matrices X and Y
X = normal(X)
Y = normal(Y)

# Check time
pr = cProfile.Profile()
pr.enable()

def pls_fit(X, Y, n_comp, tol):
    """Function to get the Partial Least Square Regression Coefficient
    X is the matrix of independent variables
    Y is the matrix of dependent variables
    tol is the convergence bound
    n_comp is the number of component"""
    
    # import package
    import numpy as np
    from numpy import linalg as LA
    
    # get variable numbers of X and Y
    n = X.shape[0]
    m = X.shape[1]
    k = Y.shape[1]
    
    # normalize the matrices X and Y
    X = normal(X)
    Y = normal(Y)
    
    # set initial value for the scores of Y and X
    E = X
    F = Y
    u = Y[:, 0][:, None]
    t = X[:, 0][:, None]
    W = np.zeros((m, n_comp))
    Q = np.zeros((k, n_comp))
    P = np.zeros((m, n_comp))
    B = np.zeros(n_comp)
    U = np.zeros((n, n_comp))
    T = np.zeros((n, n_comp))
    
    def f1(u, E):
        "Function to calculate w and t"
        w = E.T @ u / (u.T @ u)
        w = w/LA.norm(w)
        t = E @ w /(w.T @ w)
        return (w, t)
    
    # The PLS algorithm
    # check the residuals of E and F
    for i in range(n_comp):
        # check the convergency of t (the scores of X)
        if k == 1:
            w = f1(Y, E)[0]
            t = f1(Y, E)[1]
        else: 
            while np.allclose(t, f1(u, E)[1], atol = tol, rtol = tol) == False:
                w = f1(u, E)[0]
                t = f1(u, E)[1]
                q = F.T @ t/(t.T @ t)
                q = q/LA.norm(q)
                u = F @ q /(q.T @ q)
                
        p = E.T @ t/(t.T @ t)
        t = t * LA.norm(p)        
        w = w * LA.norm(p)
        p = p/LA.norm(p)
        b = u.T @ t/(t.T @ t)
        
        E = E - t @ p.T
        if k == 1:
            F = F - b * t
        else:
            F = F - b * t @ q.T
            Q[:, i][:, None] = q
            
        W[:, i][:, None] = w     
        P[:, i][:, None] = p
        B[i] = b
        U[:, i][:, None] = u 
        T[:, i][:, None] = t 
    
    if k == 1:
        Q = np.ones((1, n_comp))
    
    return m, k, B, W, Q, P, U, T

def pls_prediction(X_train, Y_train, X_test, n_comp, tol):
    """Function to get the Partial Least Square Regression Prediction
    X is the matrix of independent variables
    Y is the matrix of dependent variables
    tol is the convergence bound
    n_comp is the number of component"""
    
    # import package
    import numpy as np
    
    # normalize the matrices X and Y
    X_test = normal(X_test)
    
    # get variable numbers of X and Y
    n = X_test.shape[0]
    
    m, k, B, W, Q, P = pls_fit(X_train, Y_train, n_comp, tol)[:6]
    
    # The prediction
    E = X_test
    T_hat = np.zeros((n, n_comp))
    Y_pred = np.zeros((n, k))
    
    if n_comp > m:
        print("Error: number of components is larger than number of X variables")
    else:
        for i in range(n_comp):
            T_hat[:, i] = E @ W[:, i]
            E = E - T_hat[:, i][:, None] @ P[:, i][None, :]
            if k == 1:
                Y_pred += B[i] * T_hat[:, i][:, None]
            else:
                Y_pred += B[i] * T_hat[:, i][:, None] @ Q[:, i][None, :]
    
    return Y_pred

pls_prediction(X, Y, X, 3, 1e-06)
    
pr.disable()
s = StringIO()
sortby = 'cumulative'
ps = pstats.Stats(pr, stream = s).sort_stats(sortby)
ps.print_stats()
print(s.getvalue())
